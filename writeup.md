##Vehicle Detection 

This project detects vehicles in a given video stream of highway data. 

While there are  multiple ways to do this, this implementation focuses on an approach with HOG feature extraction + SVM classifier training. 

All the code for this project is contained in the `code.py` file. 

## Inspect Data

The code starts off with a quick visual inspection of the data 

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image1.JPG)

## HOG Features

The HOG features are extracted with the help of the `get_hog_features` function provided in the Udacity lessons. Using some test code to sweep through the various HOG extraction parameeters (orient, pix_per_cell and cells_per_block), I looked at a few different values of the parameters.

Some sample data is given below for both vehicle and non-vehicle data. The title (x,y,z) of each picture shows the values of (orient, pix_per_cell and cells_per_block)

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image2.JPG)

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image3.JPG)

I also explored different color spaces and ended up picking YUV color space with orientation of 11, pix_per_cell of 16 and cell_per_block of 2. After multiple trial runs, there were also many other combinations that produced somewhat similar result so I'm not sure that the values I picked were necessarily the most optimal. 

For initial build of video extraction pipeline, I decided to use only the HOG feature extraction. The color histogram/spatial color info were additional tools I could come back to and rely on if the HOG features did not provide sufficient accuracy by themselves.

## SVC Classifier

The SVC was extremely simple to create and train using the extracted feature data, requiring essentially only 7 lines of code:

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image4.JPG)

The SVC was able to achieve a test data accuracy of 0.9837 for classification. 

## Sliding Window Search

Using the guidelines from the lessons, I used small scale windows near the middle of the image to capture distant vehicles, and used progressively larger windows while going towards the bottom of the image. 

The choices of the bounding boxes and scales were mostly a trial and error experimentation. I ended up with scales of 1, 1.5, 2 and 3.5 and the windows generated by them look like these:

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image5.JPG)

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image6.JPG)

## Heatmaps

To reduce false positives, a threholded heatmap based filtering technique is applied that amplifies multiple overlaps of a detected vehicle.

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image8.JPG)

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image9.JPG)

## Pipeline Output

The following images are examples of test images processed through the pipeline. A reasonably good detection method is provided by the implemented pipeline!

![Image](https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/examples/image7.JPG)

## Video Implementation

Here is a sample video generated by the pipeline. The basic detection works, although the video is a bit wobbly across frames.

https://github.com/kiranganesh/CarND-Vehicle-Detection/blob/master/test_video_out.mp4

## Discussion

This project implements a very basic vehicle detection algorithm. 

The simplest improvement that can be done on this is the implementation of smoothing the resulting output over past frames (so that each frame is not processed completely independently of past detections) This would make the video a whole lot smoother.

A more comprehensive improvement would be to explore alternative methods (deep learning with CNNs, for example) that achieve the same result. 



